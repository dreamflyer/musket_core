<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Reference - Musket ML</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reference";
    var mkdocs_page_input_path = "segmentation\\reference.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Musket ML</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Generic</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../generic/">User guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../../generic/reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Segmentation</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../">User guide</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Reference</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#segmentation-pipeline-reference">Segmentation pipeline reference</a></li>
    

    <li class="toctree-l3"><a href="#pipeline-root-properties">Pipeline root properties</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#activation">activation</a></li>
        
            <li><a class="toctree-l4" href="#aggregation_metric">aggregation_metric</a></li>
        
            <li><a class="toctree-l4" href="#architecture">architecture</a></li>
        
            <li><a class="toctree-l4" href="#augmentation">augmentation</a></li>
        
            <li><a class="toctree-l4" href="#backbone">backbone</a></li>
        
            <li><a class="toctree-l4" href="#batch">batch</a></li>
        
            <li><a class="toctree-l4" href="#classifier">classifier</a></li>
        
            <li><a class="toctree-l4" href="#classifier_lr">classifier_lr</a></li>
        
            <li><a class="toctree-l4" href="#classes">classes</a></li>
        
            <li><a class="toctree-l4" href="#callbacks">callbacks</a></li>
        
            <li><a class="toctree-l4" href="#compresspredictionsasints">compressPredictionsAsInts</a></li>
        
            <li><a class="toctree-l4" href="#copyweights">copyWeights</a></li>
        
            <li><a class="toctree-l4" href="#clipnorm">clipnorm</a></li>
        
            <li><a class="toctree-l4" href="#clipvalue">clipvalue</a></li>
        
            <li><a class="toctree-l4" href="#crops">crops</a></li>
        
            <li><a class="toctree-l4" href="#dataset">dataset</a></li>
        
            <li><a class="toctree-l4" href="#datasets">datasets</a></li>
        
            <li><a class="toctree-l4" href="#dataset_augmenter">dataset_augmenter</a></li>
        
            <li><a class="toctree-l4" href="#dropout">dropout</a></li>
        
            <li><a class="toctree-l4" href="#encoder_weights">encoder_weights</a></li>
        
            <li><a class="toctree-l4" href="#extra_train_data">extra_train_data</a></li>
        
            <li><a class="toctree-l4" href="#folds_count">folds_count</a></li>
        
            <li><a class="toctree-l4" href="#freeze_encoder">freeze_encoder</a></li>
        
            <li><a class="toctree-l4" href="#final_metrics">final_metrics</a></li>
        
            <li><a class="toctree-l4" href="#holdout">holdout</a></li>
        
            <li><a class="toctree-l4" href="#imports">imports</a></li>
        
            <li><a class="toctree-l4" href="#inference_batch">inference_batch</a></li>
        
            <li><a class="toctree-l4" href="#loss">loss</a></li>
        
            <li><a class="toctree-l4" href="#lr">lr</a></li>
        
            <li><a class="toctree-l4" href="#manualresize">manualResize</a></li>
        
            <li><a class="toctree-l4" href="#metrics">metrics</a></li>
        
            <li><a class="toctree-l4" href="#num_seeds">num_seeds</a></li>
        
            <li><a class="toctree-l4" href="#optimizer">optimizer</a></li>
        
            <li><a class="toctree-l4" href="#primary_metric">primary_metric</a></li>
        
            <li><a class="toctree-l4" href="#primary_metric_mode">primary_metric_mode</a></li>
        
            <li><a class="toctree-l4" href="#preprocessing">preprocessing</a></li>
        
            <li><a class="toctree-l4" href="#random_state">random_state</a></li>
        
            <li><a class="toctree-l4" href="#shape">shape</a></li>
        
            <li><a class="toctree-l4" href="#stages">stages</a></li>
        
            <li><a class="toctree-l4" href="#stratified">stratified</a></li>
        
            <li><a class="toctree-l4" href="#testsplit">testSplit</a></li>
        
            <li><a class="toctree-l4" href="#testsplitseed">testSplitSeed</a></li>
        
            <li><a class="toctree-l4" href="#testtimeaugmentation">testTimeAugmentation</a></li>
        
            <li><a class="toctree-l4" href="#transforms">transforms</a></li>
        
            <li><a class="toctree-l4" href="#validationsplit">validationSplit</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#callback-types">Callback types</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#earlystopping">EarlyStopping</a></li>
        
            <li><a class="toctree-l4" href="#reducelronplateau">ReduceLROnPlateau</a></li>
        
            <li><a class="toctree-l4" href="#cycliclr">CyclicLR</a></li>
        
            <li><a class="toctree-l4" href="#lrvariator">LRVariator</a></li>
        
            <li><a class="toctree-l4" href="#tensorboard">TensorBoard</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#stage-properties">Stage properties</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#loss_1">loss</a></li>
        
            <li><a class="toctree-l4" href="#lr_1">lr</a></li>
        
            <li><a class="toctree-l4" href="#initial_weights">initial_weights</a></li>
        
            <li><a class="toctree-l4" href="#epochs">epochs</a></li>
        
            <li><a class="toctree-l4" href="#unfreeze_encoder">unfreeze_encoder</a></li>
        
            <li><a class="toctree-l4" href="#callbacks_1">callbacks</a></li>
        
            <li><a class="toctree-l4" href="#extra_callbacks">extra_callbacks</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#preprocessors">Preprocessors</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#cache">cache</a></li>
        
            <li><a class="toctree-l4" href="#disk-cache">disk-cache</a></li>
        
            <li><a class="toctree-l4" href="#split-preprocessor">split-preprocessor</a></li>
        
            <li><a class="toctree-l4" href="#split-concat-preprocessor">split-concat-preprocessor</a></li>
        
            <li><a class="toctree-l4" href="#seq-preprocessor">seq-preprocessor</a></li>
        
            <li><a class="toctree-l4" href="#augmentation_1">augmentation</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Classification</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../classification/">User guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../../classification/reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Musket ML</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Segmentation &raquo;</li>
        
      
    
    <li>Reference</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="segmentation-pipeline-reference">Segmentation pipeline reference</h1>
<h1 id="pipeline-root-properties">Pipeline root properties</h1>
<h2 id="activation">activation</h2>
<p><strong>type</strong>: <code>string</code></p>
<p>Activation function that should be used in last layer. In the case of binary segmentation it usually should be <code>sigmoid</code> if you have
more then one class than most likely you need to use <code>softmax</code>, but actually you are free to use any activation function that is
registered in Keras</p>
<p>Example:</p>
<pre><code class="yaml">activation: sigmoid
</code></pre>

<h2 id="aggregation_metric">aggregation_metric</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Metric to calculate against the combination of all stages and report in <code>allStages</code> section of summary.yaml file after all experiment instances are finished.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively. </p>
<p>Example:</p>
<pre><code class="yaml">aggregation_metric: matthews_correlation_holdout

</code></pre>

<h2 id="architecture">architecture</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>This property configures decoder architecture that should be used:</p>
<p>At this moment segmentation pipeline supports following architectures:</p>
<ul>
<li><a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">Unet</a></li>
<li><a href="https://codeac29.github.io/projects/linknet/">Linknet</a></li>
<li><a href="https://arxiv.org/abs/1612.01105">PSP</a></li>
<li><a href="https://arxiv.org/abs/1612.03144">FPN</a></li>
<li><a href="https://arxiv.org/abs/1706.05587">DeeplabV3</a></li>
</ul>
<p>Example:</p>
<pre><code class="yaml">architecture: FPN
</code></pre>

<h2 id="augmentation">augmentation</h2>
<p><strong>type</strong>: <code>complex</code> </p>
<p><a href="https://imgaug.readthedocs.io">IMGAUG</a> transformations sequence.
Each object is mapped on <a href="https://imgaug.readthedocs.io">IMGAUG</a> transformer by name, parameters are mapped too.</p>
<p>Example:</p>
<pre><code class="yaml">transforms:
 Fliplr: 0.5
 Affine:
   translate_px:
     x:
       - -50
       - +50
     y:
       - -50
       - +50
</code></pre>

<h2 id="backbone">backbone</h2>
<p><strong>type</strong>: <code>string</code></p>
<p>This property configures encoder that should be used:</p>
<p><code>FPN</code>, <code>PSP</code>, <code>Linkenet</code>, <code>UNet</code> architectures support following backbones: </p>
<ul>
<li><a href="https://arxiv.org/abs/1409.1556">VGGNet</a><ul>
<li>vgg16</li>
<li>vgg19</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1512.03385">ResNet</a><ul>
<li>resnet18</li>
<li>resnet34</li>
<li>resnet50 </li>
<li>resnet101</li>
<li>resnet152</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1611.05431">ResNext</a><ul>
<li>resnext50</li>
<li>resnext101</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1608.06993">DenseNet</a><ul>
<li>densenet121</li>
<li>densenet169</li>
<li>densenet201</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1512.00567">Inception-v3</a></li>
<li><a href="https://arxiv.org/abs/1602.07261">Inception-ResNet-v2</a></li>
</ul>
<p>All them support the weights pretrained on <a href="http://www.image-net.org/">ImageNet</a>:</p>
<pre><code class="yaml">encoder_weights: imagenet
</code></pre>

<p>At this moment <code>DeeplabV3</code> architecture supports following backbones:
 - <a href="https://arxiv.org/abs/1801.04381">MobileNetV2</a>
 - <a href="https://arxiv.org/abs/1610.02357">Xception</a></p>
<p>Deeplab supports weights pretrained on <a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>:</p>
<h2 id="batch">batch</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Sets up training batch size.</p>
<p>Example:</p>
<pre><code class="yaml">batch: 512
</code></pre>

<h2 id="classifier">classifier</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>TODO description</p>
<p>Supported values:
- <a href="https://arxiv.org/abs/1512.03385">ResNet</a>
    - resnet50
<a href="https://arxiv.org/abs/1608.06993">DenseNet</a>
    - densenet121
    - densenet169
    - densenet201</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="classifier_lr">classifier_lr</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>TODO description</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="classes">classes</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of classes that should be segmented.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="callbacks">callbacks</h2>
<p><strong>type</strong>: <code>array of callback instances</code> </p>
<p>Sets up training-time callbacks. See individual <a href="#callback-types">callback descriptions</a>.</p>
<p>Example:</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h2 id="compresspredictionsasints">compressPredictionsAsInts</h2>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to represent predictions as integers (up to 4 channels)
TODO check this is correct</p>
<p>Example:</p>
<pre><code class="yaml">compressPredictionsAsInts: true
</code></pre>

<h2 id="copyweights">copyWeights</h2>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to copy saved weights.</p>
<p>Example:</p>
<pre><code class="yaml">copyWeights: true
</code></pre>

<h2 id="clipnorm">clipnorm</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Maximum clip norm of a gradient for an optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">clipnorm: 1.0
</code></pre>

<h2 id="clipvalue">clipvalue</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Clip value of a gradient for an optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">clipvalue: 0.5
</code></pre>

<h2 id="crops">crops</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of crops to make from original image.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="dataset">dataset</h2>
<p><strong>type</strong>: <code>complex object</code> </p>
<p>Key is a name of the python function in scope, which returns training data set.
Value is an array of parameters to pass to a function.</p>
<p>Example:</p>
<pre><code class="yaml">dataset:
  getTrain: [false,false]
</code></pre>

<h2 id="datasets">datasets</h2>
<p><strong>type</strong>: <code>map containing complex objects</code> </p>
<p>Sets up a list of available data sets to be referred by other entities.</p>
<p>For each object, key is a name of the python function in scope, which returns training dataset.
Value is an array of parameters to pass to a function.</p>
<p>Example:</p>
<pre><code class="yaml">datasets:
  test:
    getTest: [false,false]
</code></pre>

<h2 id="dataset_augmenter">dataset_augmenter</h2>
<p><strong>type</strong>: <code>complex object</code> </p>
<p>Sets up a custom augmenter function to be applied to a dataset.
Object must have a name property, whic will be used as a name of the python function in scope.
Other object properties are mapped as function arguments.</p>
<p>Example:</p>
<pre><code class="yaml">dataset_augmenter:
    name: TheAugmenter
    parameter: test
</code></pre>

<h2 id="dropout">dropout</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="encoder_weights">encoder_weights</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>This property configures initial weights of the encoder, supported values:</p>
<p><code>imagenet</code></p>
<p>Example:</p>
<pre><code class="yaml">encoder_weights: imagenet
</code></pre>

<h2 id="extra_train_data">extra_train_data</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Name of the additional dataset that will be added (per element) to the training dataset before train launching.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="folds_count">folds_count</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of folds to train. Default is 5.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="freeze_encoder">freeze_encoder</h2>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to freeze encoder during the training process.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="final_metrics">final_metrics</h2>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Metrics to calculate against every stage and report in <code>stages</code> section of summary.yaml file after all experiment instances are finished.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">final_metrics: [measure]

</code></pre>

<h2 id="holdout">holdout</h2>
<p><strong>type</strong>: ```` </p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="imports">imports</h2>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Imports python files from <code>modules</code> folder of the project and make their properly annotated contents to be available to be referred from YAML.</p>
<p>Example:</p>
<pre><code class="yaml">imports: [ layers, preprocessors ]

</code></pre>

<p>this will import <code>layers.py</code> and <code>preprocessors.py</code></p>
<h2 id="inference_batch">inference_batch</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Size of batch during inferring process.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="loss">loss</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the loss name.</p>
<p>Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules.</p>
<p>Example:</p>
<pre><code class="yaml">loss: binary_crossentropy
</code></pre>

<h2 id="lr">lr</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Learning rate.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="manualresize">manualResize</h2>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>TODO</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="metrics">metrics</h2>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Array of metrics to track during the training process. Metric calculation results will be printed in the console and to <code>metrics</code> folder of the experiment.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">metrics: #We would like to track some metrics
  - binary_accuracy
  - binary_crossentropy
  - matthews_correlation

</code></pre>

<h2 id="num_seeds">num_seeds</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>If set, training process (for all folds) will be executed <code>num_seeds</code> times, each time resetting the random seeds.
Respective folders (like <code>metrics</code>) will obtain subfolders <code>0</code>, <code>1</code> etc... for each seed.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="optimizer">optimizer</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">optimizer: Adam
</code></pre>

<h2 id="primary_metric">primary_metric</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Metric to track during the training process. Metric calculation results will be printed in the console and to <code>metrics</code> folder of the experiment.</p>
<p>Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">primary_metric: val_macro_f1
</code></pre>

<h2 id="primary_metric_mode">primary_metric_mode</h2>
<p><strong>type</strong>: <code>enum: auto,min,max</code> </p>
<p><strong>default</strong>: <code>auto</code> </p>
<p>In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result.</p>
<p>Example:</p>
<pre><code class="yaml">primary_metric_mode: max
</code></pre>

<h2 id="preprocessing">preprocessing</h2>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Preprocessors are the custom python functions that transform dataset. </p>
<p>Such functions should be defined in python files that are in a project scope (<code>modules</code>) folder and imported.
Preprocessing functions should be also marked with <code>@preprocessing.dataset_preprocessor</code> annotation.</p>
<p><code>preprocessing</code> instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments.</p>
<p><a href="#preprocessors">Preprocessors</a> contain some of the preprocessor utility instructions.</p>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h2 id="random_state">random_state</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>The seed of randomness.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="shape">shape</h2>
<p><strong>type</strong>: <code>array of integers</code> </p>
<p>Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing</p>
<p>Example:</p>
<pre><code class="yaml">shape: [440,440,3]
</code></pre>

<h2 id="stages">stages</h2>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Sets up training process stages. 
Contains YAML array of stages, where each stage is a complex type that may contain properties described in the <a href="#stage-properties">Stage properties</a> section.  </p>
<p>Example:</p>
<pre><code class="yaml">stages:
  - epochs: 6
  - epochs: 6
    lr: 0.01

</code></pre>

<h2 id="stratified">stratified</h2>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to use stratified strategy when splitting training set.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="testsplit">testSplit</h2>
<p><strong>type</strong>: <code>float 0-1</code> </p>
<p>Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing.
The split is shuffled.</p>
<p>Example:</p>
<pre><code class="yaml">testSplit: 0.4
</code></pre>

<h2 id="testsplitseed">testSplitSeed</h2>
<p><strong>type</strong>: ```` </p>
<p>Seed of randomness for the split of the training set.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="testtimeaugmentation">testTimeAugmentation</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Test-time augumentation function name.
Function must be reachable on project scope, accept and return numpy array.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="transforms">transforms</h2>
<p><strong>type</strong>: <code>complex</code> </p>
<p>If yes, why are we having pure IMGAUG in generic called just "transforms", maybe we should call it "imageTransforms" or simply "imgaug". 
Btw, isnt it crossing with preprocessing, maybe we should just create "imgaug" preprocessor with all these goodies inside? </p>
<p><a href="https://imgaug.readthedocs.io">IMGAUG</a> transformations sequence.
Each object is mapped on <a href="https://imgaug.readthedocs.io">IMGAUG</a> transformer by name, parameters are mapped too.</p>
<p>Example:</p>
<pre><code class="yaml">transforms:
 Fliplr: 0.5
 Affine:
   translate_px:
     x:
       - -50
       - +50
     y:
       - -50
       - +50
</code></pre>

<h2 id="validationsplit">validationSplit</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h1 id="callback-types">Callback types</h1>
<h2 id="earlystopping">EarlyStopping</h2>
<p>Stop training when a monitored metric has stopped improving.</p>
<p>Properties:</p>
<ul>
<li><strong>patience</strong> - integer, number of epochs with no improvement after which training will be stopped.</li>
<li><strong>verbose</strong> - 0 or 1, verbosity mode.</li>
<li><strong>monitor</strong> - string, name of the metric to monitor</li>
<li><strong>mode</strong> - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. </li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
</code></pre>

<h2 id="reducelronplateau">ReduceLROnPlateau</h2>
<p>Reduce learning rate when a metric has stopped improving.</p>
<p>Properties:</p>
<ul>
<li><strong>patience</strong> - integer, number of epochs with no improvement after which training will be stopped.</li>
<li><strong>cooldown</strong> - integer, number of epochs to wait before resuming normal operation after lr has been reduced.</li>
<li><strong>factor</strong> - number, factor by which the learning rate will be reduced. new_lr = lr * factor</li>
<li><strong>verbose</strong> - 0 or 1, verbosity mode.</li>
<li><strong>monitor</strong> - string, name of the metric to monitor</li>
<li><strong>mode</strong> - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h2 id="cycliclr">CyclicLR</h2>
<p>Cycles learning rate across epochs.</p>
<p>Functionally, it defines the cycle amplitude (max_lr - base_lr).
The lr at any cycle is the sum of base_lr
and some scaling of the amplitude; therefore
max_lr may not actually be reached depending on
scaling function.</p>
<p>Properties:</p>
<ul>
<li><strong>base_lr</strong> - number, initial learning rate which is the lower boundary in the cycle.</li>
<li><strong>max_lr</strong> - number, upper boundary in the cycle.</li>
<li><strong>mode</strong> - one of <code>triangular</code>, <code>triangular2</code> or <code>exp_range</code>; scaling function.</li>
<li><strong>gamma</strong> - number from 0 to 1, constant in 'exp_range' scaling function.</li>
<li><strong>step_size</strong> - integer &gt; 0, number of training iterations (batches) per half cycle.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  CyclicLR:
    base_lr: 0.001
    max_lr: 0.006
    step_size: 2000
    mode: triangular
</code></pre>

<h2 id="lrvariator">LRVariator</h2>
<p>Changes learning rate between two values</p>
<p>Properties:</p>
<ul>
<li><strong>fromVal</strong> - initial learning rate value, defaults to the configuration LR setup.</li>
<li><strong>toVal</strong> - final learning value.</li>
<li><strong>style</strong> - one of the following:</li>
<li><strong>linear</strong> - changes LR linearly between two values.</li>
<li><strong>const</strong> - does not change from initial value.</li>
<li><strong>cos+</strong> - <code>-1 * cos(2x/pi) + 1 for x in [0;1]</code></li>
<li><strong>cos-</strong> - <code>cos(2x/pi) for x in [0;1]</code></li>
<li><strong>cos</strong> - same as 'cos-'</li>
<li><strong>sin+</strong> - <code>sin(2x/pi) x in [0;1]</code></li>
<li><strong>sin-</strong> - <code>-1 * sin(2x/pi) + 1 for x in [0;1]</code></li>
<li><strong>sin</strong> - same as 'sin+'</li>
<li><strong>any positive float or integer value</strong> - x^a for x in [0;1]</li>
</ul>
<p>Example</p>
<pre><code class="yaml"></code></pre>

<h2 id="tensorboard">TensorBoard</h2>
<p>This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.</p>
<p>Properties:</p>
<ul>
<li><strong>log_dir</strong> - string; the path of the directory where to save the log files to be parsed by TensorBoard.</li>
<li><strong>histogram_freq</strong> - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.</li>
<li><strong>batch_size</strong> - integer; size of batch of inputs to feed to the network for histograms computation.</li>
<li><strong>write_graph</strong> - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.</li>
<li><strong>write_grads</strong> - boolean; whether to visualize gradient histograms in TensorBoard.  histogram_freq must be greater than 0.</li>
<li><strong>write_images</strong> - boolean; whether to write model weights to visualize as image in TensorBoard.</li>
<li><strong>embeddings_freq</strong> - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data.</li>
<li><strong>embeddings_layer_names</strong> - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched.</li>
<li><strong>embeddings_metadata</strong> - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</li>
<li><strong>embeddings_data</strong> -  data to be embedded at layers specified in  embeddings_layer_names. </li>
<li><strong>update_freq</strong> - <code>epoch</code> or <code>batch</code> or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  TensorBoard:
    log_dir: './logs'
    batch_size: 32
    write_graph: True
    update_freq: batch
</code></pre>

<h1 id="stage-properties">Stage properties</h1>
<h2 id="loss_1">loss</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the loss name.</p>
<p>Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules.</p>
<p>Example:</p>
<pre><code class="yaml">loss: binary_crossentropy
</code></pre>

<h2 id="lr_1">lr</h2>
<p><strong>type</strong>: <code>float</code> </p>
<p>Learning rate.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="initial_weights">initial_weights</h2>
<p><strong>type</strong>: <code>string</code> </p>
<p>Fil path to load stage NN initial weights from.</p>
<p>Example:</p>
<pre><code class="yaml">initial_weights: /initial.weights
</code></pre>

<h2 id="epochs">epochs</h2>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of epochs to train for this stage.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="unfreeze_encoder">unfreeze_encoder</h2>
<h2 id="callbacks_1">callbacks</h2>
<p><strong>type</strong>: <code>array of callback instances</code> </p>
<p>Sets up training-time callbacks. See individual <a href="#callback-types">callback descriptions</a>.</p>
<p>Example:</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h2 id="extra_callbacks">extra_callbacks</h2>
<h1 id="preprocessors">Preprocessors</h1>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Preprocessors are the custom python functions that transform dataset. </p>
<p>Such functions should be defined in python files that are in a project scope (<code>modules</code>) folder and imported.
Preprocessing functions should be also marked with <code>@preprocessing.dataset_preprocessor</code> annotation.</p>
<p><code>Preprocessors</code> instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments.</p>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h2 id="cache">cache</h2>
<p>Caches its input.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="disk-cache">disk-cache</h2>
<p>Caches its input on disk, including the full flow. 
On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. </p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h2 id="split-preprocessor">split-preprocessor</h2>
<p>An analogue of <a href="#split">split</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="split-concat-preprocessor">split-concat-preprocessor</h2>
<p>An analogue of <a href="#split-concat">split-concat</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="seq-preprocessor">seq-preprocessor</h2>
<p>An analogue of <a href="#seq-concat">seq</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="augmentation_1">augmentation</h2>
<p>Preprocessor instruction, which body only runs during the training and is skipped when the inferring.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../classification/" class="btn btn-neutral float-right" title="User guide">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../" class="btn btn-neutral" title="User guide"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../classification/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
